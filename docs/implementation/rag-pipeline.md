# Step 1: RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„ - ë‚˜ë§Œì˜ AI ê²€ìƒ‰ì—”ì§„ ë§Œë“¤ê¸°

> **ëª©í‘œ**: "ì—…ë¡œë“œ â†’ ì„ë² ë”© â†’ ê²€ìƒ‰ â†’ ë‹µë³€" í”Œë¡œìš°ë¥¼ FastAPI + LangChain + Chromaë¡œ ì²˜ìŒë¶€í„° ëê¹Œì§€ êµ¬ì¶•í•œë‹¤.

---

## ğŸ¯ ì´ ë‹¨ê³„ë¥¼ ë°°ìš°ëŠ” ì´ìœ 

### ì™œ RAG(Retrieval-Augmented Generation)ì¸ê°€?

**RAGëŠ” "ë‚˜ë§Œì˜ ChatGPT"ë¥¼ ë§Œë“œëŠ” ê¸°ìˆ **ì…ë‹ˆë‹¤.

ì¼ë°˜ ChatGPTì˜ í•œê³„:
- âŒ íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œëŠ” ëª¨ë¦…ë‹ˆë‹¤ (í•™ìŠµ ì•ˆ ë¨)
- âŒ ìµœì‹  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ (í•™ìŠµ ì‹œì  ì´í›„ ë°ì´í„° ë¶€ì¬)
- âŒ ì¶œì²˜ë¥¼ ì œì‹œí•˜ì§€ ëª»í•©ë‹ˆë‹¤ (í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜)

RAG ì±—ë´‡ì˜ ê°•ì :
- âœ… ìš°ë¦¬ íšŒì‚¬ ë¬¸ì„œë¡œ ë‹µë³€ (PDF, Docx, CSV ì—…ë¡œë“œ)
- âœ… í•­ìƒ ìµœì‹  ì •ë³´ (ë¬¸ì„œë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ë¨)
- âœ… ì¶œì²˜ ëª…ì‹œ (í˜ì´ì§€ ë²ˆí˜¸, URL ì œê³µ)

### í”„ë¦¬ëœì„œ ì…ì¥ì—ì„œ ì™œ í•„ìš”í•œê°€?

1. **ì¤‘ì†Œê¸°ì—…ì˜ ì‹¤ì œ ë‹ˆì¦ˆ**
   - "ì‚¬ë‚´ ê·œì •ì§‘ì´ 200í˜ì´ì§€ì¸ë° ì§ì›ë“¤ì´ ì•ˆ ì½ì–´ìš”"
   - "ê³ ê° ë¬¸ì˜ê°€ ë°˜ë³µë˜ëŠ”ë° ì¼ì¼ì´ ë‹µë³€í•˜ê¸° í˜ë“¤ì–´ìš”"
   - "ì‹ ì…ì‚¬ì› êµìœ¡ ìë£Œë¥¼ ì±—ë´‡ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‚˜ìš”?"

2. **ì™¸ì£¼ ë‹¨ê°€ê°€ ë†’ìŠµë‹ˆë‹¤**
   - ë‹¨ìˆœ í™ˆí˜ì´ì§€: 200~500ë§Œì›
   - **RAG ì±—ë´‡: 800~2,000ë§Œì›** (AI ê¸°ìˆ ë ¥ í”„ë¦¬ë¯¸ì—„)
   - ì›” ìœ ì§€ë³´ìˆ˜: 50~200ë§Œì›

3. **ê²½ìŸìê°€ ì ìŠµë‹ˆë‹¤**
   - ëŒ€ë¶€ë¶„ì€ "OpenAI API ë¶™ì´ê¸°"ë§Œ í•©ë‹ˆë‹¤
   - ë¬¸ì„œ ê´€ë¦¬, ê¶Œí•œ, ë¹„ìš© ìµœì í™”ê¹Œì§€ í•˜ëŠ” ì‚¬ëŒì€ ë“œë­…ë‹ˆë‹¤

### ì´ ë¬¸ì„œë¥¼ ëë‚´ë©´ ì´ë ‡ê²Œ ë©ë‹ˆë‹¤

**Before (ì¼ë°˜ ê°œë°œì)**
```
ê³ ê°: "ìš°ë¦¬ íšŒì‚¬ ê·œì •ì§‘ ì±—ë´‡ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
ê°œë°œì: "ChatGPT API ì—°ê²°í•´ë“œë¦´ê²Œìš”."
ê³ ê°: "ê·¸ëŸ¼ ì–´ë–»ê²Œ ìš°ë¦¬ ë¬¸ì„œë¥¼ í•™ìŠµì‹œí‚¤ë‚˜ìš”?"
ê°œë°œì: "...?" ğŸ’¥
```

**After (ì´ ë¬¸ì„œ í•™ìŠµ í›„)**
```
ê³ ê°: "ìš°ë¦¬ íšŒì‚¬ ê·œì •ì§‘ ì±—ë´‡ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
ì—¬ëŸ¬ë¶„: "PDF ì—…ë¡œë“œí•˜ì‹œë©´ ìë™ìœ¼ë¡œ ìƒ‰ì¸ë©ë‹ˆë‹¤.
       ì¶œì²˜ë„ í‘œì‹œë˜ê³ , ê¶Œí•œë³„ë¡œ ì ‘ê·¼ ì œì–´ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
       ë°ëª¨ ë³´ì—¬ë“œë¦´ê¹Œìš”?" âœ¨
```

---

## ğŸ’¡ í•µì‹¬ ê°œë… ë¨¼ì € ì´í•´í•˜ê¸°

### 1. RAGê°€ ë­ì•¼?

**ì „í†µì ì¸ ChatGPT**
```
ì§ˆë¬¸ â†’ GPT â†’ ë‹µë³€ (í•™ìŠµëœ ì§€ì‹ë§Œ ì‚¬ìš©)
```

**RAG ë°©ì‹**
```
ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ â†’ ê²€ìƒ‰ ê²°ê³¼ + ì§ˆë¬¸ â†’ GPT â†’ ë‹µë³€
```

**ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°:**
- ì¼ë°˜ ChatGPT = ëª¨ë“  ê±¸ ì™¸ìš°ê³  ì‹œí—˜ ë³´ëŠ” í•™ìƒ
- RAG = ìë£Œ ê²€ìƒ‰í•´ì„œ ë‹µë³€í•˜ëŠ” í•™ìƒ (ì˜¤í”ˆë¶ ì‹œí—˜)

### 2. ì„ë² ë”©(Embedding)ì´ë€?

**ë¬¸ì œ ìƒí™©:**
```python
# ì´ë ‡ê²Œ ê²€ìƒ‰í•˜ë©´ ì‘ë™ ì•ˆ í•¨
if "ë³µì§€í¬ì¸íŠ¸" in ë¬¸ì„œ:
    return ë¬¸ì„œ
# "ë³µë¦¬í›„ìƒ í¬ì¸íŠ¸"ë¼ê³  ì“°ì—¬ ìˆìœ¼ë©´? ëª» ì°¾ìŒ ğŸ’¥
```

**ì„ë² ë”©ì˜ í•´ê²°ì±…:**
```python
# ì˜ë¯¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜
"ë³µì§€í¬ì¸íŠ¸ ì‹ ì²­" â†’ [0.2, 0.8, 0.1, ...] (1536ì°¨ì›)
"ë³µë¦¬í›„ìƒ í¬ì¸íŠ¸" â†’ [0.19, 0.81, 0.11, ...] (ìœ ì‚¬í•œ ë²¡í„°)

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ â†’ 95% ì¼ì¹˜! âœ…
```

**ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€?**
- ë™ì˜ì–´, ìœ ì‚¬ í‘œí˜„ ìë™ ì²˜ë¦¬
- "ì—°ì°¨ ì‹ ì²­"ê³¼ "íœ´ê°€ ë‚´ëŠ” ë²•"ì„ ê°™ì€ ì˜ë¯¸ë¡œ ì¸ì‹

### 3. ë²¡í„° DBê°€ í•„ìš”í•œ ì´ìœ 

ì¼ë°˜ DB (MySQL, PostgreSQL):
```sql
SELECT * FROM docs WHERE title = 'ê·œì •ì§‘';  -- ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
```

ë²¡í„° DB (Chroma, Pinecone):
```python
vector_db.similarity_search("ì—°ì°¨ ì‚¬ìš© ë°©ë²•")
# â†’ "íœ´ê°€ ê´€ë¦¬ ê·œì •" ë¬¸ì„œë¥¼ ìœ ì‚¬ë„ 0.89ë¡œ ì°¾ì•„ëƒ„ âœ¨
```

### 4. ì²­í¬(Chunk)ëŠ” ì™œ ë‚˜ëˆ„ë‚˜?

**ë¬¸ì œ:**
- GPT ì…ë ¥ í† í° ì œí•œ: 4,096 ~ 128,000 í† í°
- ê·œì •ì§‘ ì „ì²´(300í˜ì´ì§€)ë¥¼ í•œ ë²ˆì— ë„£ìœ¼ë©´ ë¹„ìš© í­íƒ„ ğŸ’¸

**í•´ê²°:**
```
ê·œì •ì§‘ 300í˜ì´ì§€
â†’ 600ìì”© ì˜ë¼ì„œ 500ê°œ ì²­í¬ë¡œ ë¶„ë¦¬
â†’ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ 5ê°œ ì²­í¬ë§Œ GPTì— ì „ë‹¬
â†’ ë¹„ìš© 1/100 ì ˆê°! ğŸ‰
```

**Chunk Overlapì€ ì™œ?**
```
ì²­í¬1: "ì—°ì°¨ëŠ” ì…ì‚¬ 1ë…„ í›„ë¶€í„° 15ì¼ì´ ë¶€ì—¬ë˜ë©°"
ì²­í¬2: "ë¶€ì—¬ë˜ë©°, ì‚¬ìš©ì€ ê·¼íƒœ ì‹œìŠ¤í…œì—ì„œ ì‹ ì²­í•©ë‹ˆë‹¤"
        â†‘ 120ì ê²¹ì¹¨ (ë¬¸ë§¥ ë³´ì¡´)
```

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

| Step | ê²°ê³¼                               | í•µì‹¬ ì—­ëŸ‰                                 |
| ---- | ---------------------------------- | ----------------------------------------- |
| 1    | ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ í‘œì¤€ í¬ë§·ìœ¼ë¡œ íŒŒì‹± | íŒŒì¼ íŒŒì„œ êµ¬ì„±, ë©”íƒ€ë°ì´í„° ê´€ë¦¬           |
| 2    | í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„ë¦¬í•˜ê³  ì„ë² ë”©    | LangChain TextSplitter & OpenAI Embedding |
| 3    | ë²¡í„° DBì— ì˜êµ¬ ì €ì¥                | Chroma ì»¬ë ‰ì…˜ ê´€ë¦¬, ì¬ìƒ‰ì¸ ì „ëµ           |
| 4    | ìœ ì‚¬ë„ ê²€ìƒ‰ + GPT ì‘ë‹µ ìƒì„±        | `score_threshold`, í”„ë¡¬í”„íŠ¸ ê°€ë“œë ˆì¼      |
| 5    | ìš”ì²­/ì‘ë‹µ/ë¹„ìš© ë¡œê¹…                | ë¹„ìš© ê³„ì‚°, JSON/DB ë¡œê¹…, í’ˆì§ˆ ì§€í‘œ        |

---

## ğŸ› ï¸ ì‚¬ì „ ì¤€ë¹„

| í•­ëª©        | ë‚´ìš©                                                                                                            |
| ----------- | --------------------------------------------------------------------------------------------------------------- |
| í™˜ê²½        | Python 3.10+, Git, VS Code/PyCharm                                                                              |
| íŒ¨í‚¤ì§€      | `pip install -r requirements.txt` (ì°¸ê³ : [`appendix/setup-requirements.md`](../appendix/setup-requirements.md)) |
| í™˜ê²½ë³€ìˆ˜    | `.env`ì— `OPENAI_API_KEY`, `VECTOR_DB_PATH`, `CHUNK_SIZE`, `CHUNK_OVERLAP`                                      |
| ìƒ˜í”Œ ë°ì´í„° | `data/`ì— PDF/Docx/CSV ê° 1ê°œ ì´ìƒ                                                                              |

```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

---

## ğŸ’» ìƒ˜í”Œ ì½”ë“œ ì‹œì‘ì 

```python
# app/pipelines.py
from langchain.text_splitter import RecursiveCharacterTextSplitter
from app.vector_store import vector_client
from app.parsers import parse_document


async def process_upload(file) -> str:
    text, metadata = parse_document(file)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=120,
    )
    chunks = splitter.create_documents([text], metadatas=[metadata])
    vector_client.add_documents(chunks)
    return metadata["doc_id"]
```

```python
# app/retriever.py
from app.vector_store import vector_client
from app.llm import build_prompt_and_ask

async def retrieve_answer(question: str) -> dict:
    docs = vector_client.similarity_search(question, k=5, score_threshold=0.75)
    if not docs:
        return {
            "answer": "ìë£Œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.",
            "sources": [],
        }
    return await build_prompt_and_ask(question, docs)
```

---

## ğŸ”„ ë”°ë¼í•˜ê¸° - ì²˜ìŒë¶€í„° ëê¹Œì§€

### Step 1: í”„ë¡œì íŠ¸ í´ë” êµ¬ì¡° ë§Œë“¤ê¸°

**ëª…ë ¹ì–´ (Windows CMD)**
```bash
cd C:\Users\%USERNAME%\Desktop
mkdir rag-bot
cd rag-bot
mkdir app app\parsers data data\uploads logs tests scripts web
```

**ëª…ë ¹ì–´ (Mac/Linux)**
```bash
cd ~/Desktop
mkdir -p rag-bot/{app/parsers,data/uploads,logs,tests,scripts,web}
cd rag-bot
```

**ê²°ê³¼ í™•ì¸:**
```
rag-bot/
â”œâ”€â”€ app/          â† FastAPI ë°±ì—”ë“œ
â”œâ”€â”€ data/         â† ì—…ë¡œë“œëœ ë¬¸ì„œ
â”œâ”€â”€ logs/         â† ì‘ë‹µ ë¡œê·¸
â”œâ”€â”€ web/          â† ì›¹ ë°ëª¨ UI
â””â”€â”€ tests/        â† í…ŒìŠ¤íŠ¸ ì½”ë“œ
```

---

### Step 2: ê°€ìƒí™˜ê²½ & íŒ¨í‚¤ì§€ ì„¤ì¹˜

#### 2-1. ê°€ìƒí™˜ê²½ ìƒì„±

**Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**Mac/Linux:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**ì„±ê³µ í™•ì¸:**
```bash
# í”„ë¡¬í”„íŠ¸ ì•ì— (.venv) í‘œì‹œë˜ë©´ ì„±ê³µ
(.venv) C:\Users\...\rag-bot>
```

#### 2-2. requirements.txt ì‘ì„±

**íŒŒì¼ ìƒì„±:** `requirements.txt`

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
langchain==0.1.0
langchain-openai==0.0.2
chromadb==0.4.18
pypdf==3.17.1
python-docx==1.1.0
pandas==2.1.3
python-multipart==0.0.6
python-dotenv==1.0.0
tiktoken==0.5.2
```

**ì„¤ì¹˜:**
```bash
pip install -r requirements.txt
```

**5ë¶„ ì •ë„ ê±¸ë¦½ë‹ˆë‹¤. ì»¤í”¼ í•œ ì” â˜•**

---

### Step 3: í™˜ê²½ë³€ìˆ˜ ì„¤ì •

**íŒŒì¼ ìƒì„±:** `.env`

```env
OPENAI_API_KEY=sk-proj-ì—¬ê¸°ì—_ë³¸ì¸ì˜_í‚¤_ì…ë ¥
VECTOR_DB_PATH=./chroma
CHUNK_SIZE=600
CHUNK_OVERLAP=120
```

**OpenAI API í‚¤ ë°œê¸‰ ë°©ë²•:**
1. https://platform.openai.com/api-keys ì ‘ì†
2. "Create new secret key" í´ë¦­
3. í‚¤ ë³µì‚¬ â†’ `.env` íŒŒì¼ì— ë¶™ì—¬ë„£ê¸°
4. âš ï¸ **ì ˆëŒ€ GitHubì— ì˜¬ë¦¬ì§€ ë§ˆì„¸ìš”!** (`.gitignore`ì— `.env` ì¶”ê°€ í•„ìˆ˜)

---

### Step 4: ë¬¸ì„œ íŒŒì„œ ì‘ì„±

#### 4-1. PDF íŒŒì„œ

**íŒŒì¼ ìƒì„±:** `app/parsers/pdf.py`

```python
from pypdf import PdfReader
from typing import Tuple, Dict

def parse_pdf(file_path: str) -> Tuple[str, Dict]:
    """
    PDF íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë°˜í™˜
    
    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        
    Returns:
        (ì „ì²´ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬)
    """
    reader = PdfReader(file_path)
    
    # ëª¨ë“  í˜ì´ì§€ë¥¼ \n\nìœ¼ë¡œ êµ¬ë¶„í•´ì„œ í•©ì¹¨
    text = "\n\n".join(
        page.extract_text() 
        for page in reader.pages
    )
    
    metadata = {
        "doc_id": file_path.split("/")[-1],  # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
        "title": file_path.split("/")[-1],
        "source_path": file_path,
        "page_count": len(reader.pages),
    }
    
    return text, metadata
```

**ì™œ ì´ë ‡ê²Œ í•˜ë‚˜ìš”?**
- `\n\n`: í˜ì´ì§€ êµ¬ë¶„ì„ ëª…í™•íˆ í•´ì„œ ì²­í¬ ë¶„í•  ì‹œ í˜ì´ì§€ ê²½ê³„ ë³´ì¡´
- `metadata`: ë‚˜ì¤‘ì— ì¶œì²˜ í‘œì‹œí•  ë•Œ ì‚¬ìš© ("ê·œì •ì§‘.pdf 3í˜ì´ì§€ ì°¸ê³ ")

#### 4-2. íŒŒì¼ íƒ€ì… ë¼ìš°í„°

**íŒŒì¼ ìƒì„±:** `app/parsers/__init__.py`

```python
from pathlib import Path
from .pdf import parse_pdf

def parse_document(file_path: str):
    """
    íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ íŒŒì„œ ì„ íƒ
    """
    ext = Path(file_path).suffix.lower()
    
    if ext == ".pdf":
        return parse_pdf(file_path)
    elif ext == ".docx":
        from .docx_parser import parse_docx
        return parse_docx(file_path)
    elif ext == ".csv":
        from .csv_parser import parse_csv
        return parse_csv(file_path)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")
```

**í…ŒìŠ¤íŠ¸ ë°©ë²•:**

**íŒŒì¼ ìƒì„±:** `tests/test_parser.py`

```python
from app.parsers import parse_document

def test_pdf_parser():
    # í…ŒìŠ¤íŠ¸ìš© PDF ì¤€ë¹„ (êµ¬ê¸€ì—ì„œ "ìƒ˜í”Œ PDF" ë‹¤ìš´ë¡œë“œ)
    text, metadata = parse_document("data/sample.pdf")
    
    assert len(text) > 0, "í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
    assert metadata["page_count"] > 0, "í˜ì´ì§€ ìˆ˜ê°€ 0ì…ë‹ˆë‹¤"
    print(f"âœ… íŒŒì‹± ì„±ê³µ: {len(text)}ì, {metadata['page_count']}í˜ì´ì§€")

if __name__ == "__main__":
    test_pdf_parser()
```

**ì‹¤í–‰:**
```bash
python tests/test_parser.py
```

**ê²°ê³¼ ì˜ˆì‹œ:**
```
âœ… íŒŒì‹± ì„±ê³µ: 15243ì, 12í˜ì´ì§€
```

---

### Step 5: ì²­í¬ ë¶„í•  & ì„ë² ë”©

**íŒŒì¼ ìƒì„±:** `app/pipelines.py`

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from typing import List
import os
from dotenv import load_dotenv

load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "600"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "120"))

def split_text(text: str, metadata: dict) -> List[Document]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        text: ì „ì²´ í…ìŠ¤íŠ¸
        metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        
    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬)
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""],  # ë¬¸ë‹¨ â†’ ì¤„ â†’ ë‹¨ì–´ â†’ ë¬¸ì ìˆœ
    )
    
    chunks = splitter.create_documents(
        texts=[text],
        metadatas=[metadata]
    )
    
    print(f"âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ")
    return chunks
```

**í…ŒìŠ¤íŠ¸:**

**íŒŒì¼ ìƒì„±:** `tests/test_chunking.py`

```python
from app.parsers import parse_document
from app.pipelines import split_text

def test_chunking():
    # 1. PDF íŒŒì‹±
    text, metadata = parse_document("data/sample.pdf")
    print(f"ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸: {len(text)}ì")
    
    # 2. ì²­í¬ ë¶„í• 
    chunks = split_text(text, metadata)
    print(f"ğŸ”ª ì²­í¬ ê°œìˆ˜: {len(chunks)}ê°œ")
    
    # 3. ì²« ì²­í¬ í™•ì¸
    print(f"\nğŸ“Œ ì²« ë²ˆì§¸ ì²­í¬ (600ì):")
    print(chunks[0].page_content[:200] + "...")
    
    # 4. ì˜¤ë²„ë© í™•ì¸
    if len(chunks) >= 2:
        chunk1_end = chunks[0].page_content[-50:]
        chunk2_start = chunks[1].page_content[:50:]
        print(f"\nğŸ”— ì²­í¬1 ë: ...{chunk1_end}")
        print(f"ğŸ”— ì²­í¬2 ì‹œì‘: {chunk2_start}...")

if __name__ == "__main__":
    test_chunking()
```

**ì‹¤í–‰:**
```bash
python tests/test_chunking.py
```

**ê²°ê³¼ ì˜ˆì‹œ:**
```
ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸: 15243ì
âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: 28ê°œ
ğŸ”ª ì²­í¬ ê°œìˆ˜: 28ê°œ

ğŸ“Œ ì²« ë²ˆì§¸ ì²­í¬ (600ì):
ì œ1ì¥ ì´ì¹™
ì œ1ì¡° (ëª©ì ) ì´ ê·œì •ì€ íšŒì‚¬ì˜ ì¸ì‚¬ ê´€ë¦¬ì— ê´€í•œ ê¸°ë³¸ ì‚¬í•­ì„ ì •í•¨ìœ¼ë¡œì¨...

ğŸ”— ì²­í¬1 ë: ...ì—°ì°¨ íœ´ê°€ëŠ” ì…ì‚¬ 1ë…„ í›„ë¶€í„° 15ì¼ì´ ë¶€ì—¬ë˜ë©°
ğŸ”— ì²­í¬2 ì‹œì‘: ë¶€ì—¬ë˜ë©°, ì‚¬ìš©ì€ ê·¼íƒœ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ì‹ ì²­í•©ë‹ˆë‹¤...
```

**ğŸ‘€ í™•ì¸ í¬ì¸íŠ¸:**
- ì²­í¬ ê°œìˆ˜ê°€ ì ì ˆí•œê°€? (ë„ˆë¬´ ë§ìœ¼ë©´ ê²€ìƒ‰ ëŠë ¤ì§, ë„ˆë¬´ ì ìœ¼ë©´ ì •í™•ë„ ë–¨ì–´ì§)
- ì˜¤ë²„ë©ì´ ì œëŒ€ë¡œ ë˜ëŠ”ê°€? (ì²­í¬1 ëê³¼ ì²­í¬2 ì‹œì‘ì´ ê²¹ì³ì•¼ í•¨)

---

### Step 6: ë²¡í„° DB ìƒ‰ì¸ (Chroma)

**íŒŒì¼ ìƒì„±:** `app/vector_store.py`

```python
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document
from typing import List
import os
from dotenv import load_dotenv

load_dotenv()

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embedding = OpenAIEmbeddings(
    model="text-embedding-3-small",  # ë¹„ìš© ì ˆê°í˜•
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# Chroma ë²¡í„° DB í´ë¼ì´ì–¸íŠ¸
vector_client = Chroma(
    collection_name="rag_documents",
    embedding_function=embedding,
    persist_directory=os.getenv("VECTOR_DB_PATH", "./chroma"),
)

def add_documents(chunks: List[Document]):
    """
    ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥
    """
    vector_client.add_documents(chunks)
    vector_client.persist()  # ë””ìŠ¤í¬ì— ì €ì¥
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

def search_documents(query: str, k: int = 5):
    """
    ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    """
    results = vector_client.similarity_search_with_relevance_scores(
        query=query,
        k=k,
        score_threshold=0.75  # ìœ ì‚¬ë„ 75% ì´ìƒë§Œ
    )
    return results
```

**ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸:**

**íŒŒì¼ ìƒì„±:** `tests/test_full_pipeline.py`

```python
from app.parsers import parse_document
from app.pipelines import split_text
from app.vector_store import add_documents, search_documents

def test_full_pipeline():
    print("ğŸ“‚ Step 1: PDF íŒŒì‹±")
    text, metadata = parse_document("data/sample.pdf")
    print(f"   âœ… {len(text)}ì ì¶”ì¶œ\n")
    
    print("ğŸ”ª Step 2: ì²­í¬ ë¶„í• ")
    chunks = split_text(text, metadata)
    print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±\n")
    
    print("ğŸ—„ï¸ Step 3: ë²¡í„° DB ìƒ‰ì¸ (ì„ë² ë”© ìƒì„± ì¤‘... 30ì´ˆ ì†Œìš”)")
    add_documents(chunks)
    print(f"   âœ… ìƒ‰ì¸ ì™„ë£Œ\n")
    
    print("ğŸ” Step 4: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    query = "ì—°ì°¨ íœ´ê°€ëŠ” ëª‡ ì¼ì¸ê°€ìš”?"
    results = search_documents(query, k=3)
    
    if results:
        print(f"   âœ… {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬:")
        for doc, score in results:
            print(f"      ğŸ“„ ìœ ì‚¬ë„ {score:.2f}: {doc.page_content[:100]}...")
    else:
        print("   âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    test_full_pipeline()
```

**ì‹¤í–‰:**
```bash
python tests/test_full_pipeline.py
```

**ê²°ê³¼ ì˜ˆì‹œ:**
```
ğŸ“‚ Step 1: PDF íŒŒì‹±
   âœ… 15243ì ì¶”ì¶œ

ğŸ”ª Step 2: ì²­í¬ ë¶„í• 
   âœ… 28ê°œ ì²­í¬ ìƒì„±

ğŸ—„ï¸ Step 3: ë²¡í„° DB ìƒ‰ì¸ (ì„ë² ë”© ìƒì„± ì¤‘... 30ì´ˆ ì†Œìš”)
âœ… 28ê°œ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.
   âœ… ìƒ‰ì¸ ì™„ë£Œ

ğŸ” Step 4: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
   âœ… 3ê°œ ë¬¸ì„œ ë°œê²¬:
      ğŸ“„ ìœ ì‚¬ë„ 0.89: ì œ3ì¡° (ì—°ì°¨ íœ´ê°€) ì…ì‚¬ 1ë…„ í›„ë¶€í„° 15ì¼ì˜ ì—°ì°¨ê°€ ë¶€ì—¬ë˜ë©°...
      ğŸ“„ ìœ ì‚¬ë„ 0.82: íœ´ê°€ ì‚¬ìš©ì€ ê·¼íƒœ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ìµœì†Œ 3ì¼ ì „ ì‹ ì²­...
      ğŸ“„ ìœ ì‚¬ë„ 0.76: ë¯¸ì‚¬ìš© ì—°ì°¨ëŠ” ìµë…„ ìƒë°˜ê¸°ê¹Œì§€ ì´ì›” ê°€ëŠ¥...
```

**ğŸ‰ ì—¬ê¸°ê¹Œì§€ ì„±ê³µí–ˆë‹¤ë©´:**
- âœ… ë¬¸ì„œ ì—…ë¡œë“œ â†’ ì„ë² ë”© â†’ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì™„ì„±!
- âœ… `chroma/` í´ë”ì— ë²¡í„° DB íŒŒì¼ ìƒì„±ë¨
- âœ… ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì´ ì‘ë™í•¨ (í‚¤ì›Œë“œ ì •í™•íˆ ì¼ì¹˜ ì•ˆ í•´ë„ ì°¾ì•„ëƒ„)

---

### Step 7: GPT ì‘ë‹µ ìƒì„±

**íŒŒì¼ ìƒì„±:** `app/llm.py`

```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from typing import List, Tuple, Any
import os
from dotenv import load_dotenv

load_dotenv()

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2,  # ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì  (0~1)
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

async def generate_answer(question: str, search_results: List[Tuple[Any, float]]) -> dict:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ GPT ë‹µë³€ ìƒì„±
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        search_results: [(Document, score), ...]
        
    Returns:
        {"answer": "...", "sources": [...]}
    """
    if not search_results:
        return {
            "answer": "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì§ˆë¬¸ì„ êµ¬ì²´í™”í•´ ì£¼ì„¸ìš”.",
            "sources": []
        }
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n".join([
        f"[ë¬¸ì„œ {i+1}] {doc.page_content}"
        for i, (doc, score) in enumerate(search_results)
    ])
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
3. ë‹µë³€ ëì— ì¶œì²˜ë¥¼ bullet ë¦¬ìŠ¤íŠ¸ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
ë‹µë³€ ë‚´ìš©...

**ì¶œì²˜:**
- [ë¬¸ì„œëª…] ì°¸ê³ 
"""
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{context}")
    ]
    
    # GPT í˜¸ì¶œ
    response = await llm.apredict_messages(messages)
    
    # ì¶œì²˜ ì •ë¦¬
    sources = [
        {
            "title": doc.metadata.get("title", "ë¬¸ì„œ"),
            "page": doc.metadata.get("page_count", "?"),
            "score": round(score, 2)
        }
        for doc, score in search_results
    ]
    
    return {
        "answer": response.content,
        "sources": sources
    }
```

**í…ŒìŠ¤íŠ¸:**

**íŒŒì¼ ìƒì„±:** `tests/test_answer.py`

```python
import asyncio
from app.vector_store import search_documents
from app.llm import generate_answer

async def test_answer():
    query = "ì—°ì°¨ íœ´ê°€ ì‹ ì²­ ë°©ë²•ì€?"
    
    print(f"ğŸ” ê²€ìƒ‰ ì¤‘: {query}")
    results = search_documents(query, k=3)
    
    print(f"ğŸ¤– GPT ë‹µë³€ ìƒì„± ì¤‘...")
    answer_data = await generate_answer(query, results)
    
    print("\n" + "="*60)
    print(f"ì§ˆë¬¸: {query}")
    print("="*60)
    print(answer_data["answer"])
    print("\nì¶œì²˜:")
    for src in answer_data["sources"]:
        print(f"  - {src['title']} (ìœ ì‚¬ë„: {src['score']})")

if __name__ == "__main__":
    asyncio.run(test_answer())
```

**ì‹¤í–‰:**
```bash
python tests/test_answer.py
```

**ê²°ê³¼ ì˜ˆì‹œ:**
```
ğŸ” ê²€ìƒ‰ ì¤‘: ì—°ì°¨ íœ´ê°€ ì‹ ì²­ ë°©ë²•ì€?
ğŸ¤– GPT ë‹µë³€ ìƒì„± ì¤‘...

============================================================
ì§ˆë¬¸: ì—°ì°¨ íœ´ê°€ ì‹ ì²­ ë°©ë²•ì€?
============================================================
ì—°ì°¨ íœ´ê°€ëŠ” ê·¼íƒœ ê´€ë¦¬ ì‹œìŠ¤í…œì—ì„œ ìµœì†Œ 3ì¼ ì „ì— ì‹ ì²­í•´ì•¼ í•©ë‹ˆë‹¤. 
ì…ì‚¬ 1ë…„ í›„ë¶€í„° 15ì¼ì´ ë¶€ì—¬ë˜ë©°, ë¯¸ì‚¬ìš© ì—°ì°¨ëŠ” ìµë…„ ìƒë°˜ê¸°ê¹Œì§€ ì´ì›” ê°€ëŠ¥í•©ë‹ˆë‹¤.

**ì¶œì²˜:**
- ì¸ì‚¬ê·œì •.pdf ì°¸ê³ 
- ë³µë¦¬í›„ìƒ ê°€ì´ë“œ.pdf ì°¸ê³ 

ì¶œì²˜:
  - sample.pdf (ìœ ì‚¬ë„: 0.89)
  - sample.pdf (ìœ ì‚¬ë„: 0.82)
  - sample.pdf (ìœ ì‚¬ë„: 0.76)
```

---

### Step 8: FastAPI ì„œë²„ êµ¬ì„±

**íŒŒì¼ ìƒì„±:** `app/main.py`

```python
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from app.vector_store import search_documents
from app.llm import generate_answer
import uvicorn

app = FastAPI(title="RAG ì±—ë´‡ API")

class QuestionRequest(BaseModel):
    question: str

@app.get("/")
def health_check():
    return {"status": "ok", "message": "RAG ì±—ë´‡ ì„œë²„ ì‹¤í–‰ ì¤‘"}

@app.post("/ask")
async def ask_question(request: QuestionRequest):
    """
    ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°
    """
    try:
        # 1. ë²¡í„° ê²€ìƒ‰
        results = search_documents(request.question, k=5)
        
        # 2. GPT ë‹µë³€ ìƒì„±
        answer_data = await generate_answer(request.question, results)
        
        return JSONResponse(content=answer_data)
    
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**ì„œë²„ ì‹¤í–‰:**
```bash
python app/main.py
```

**ë¸Œë¼ìš°ì €ì—ì„œ í…ŒìŠ¤íŠ¸:**
1. http://localhost:8000 ì ‘ì† â†’ `{"status": "ok"}` í™•ì¸
2. http://localhost:8000/docs ì ‘ì† â†’ Swagger UIë¡œ `/ask` í…ŒìŠ¤íŠ¸

---

## âŒ ìì£¼ ë§Œë‚˜ëŠ” ì—ëŸ¬ì™€ í•´ê²°ë²•

### ì—ëŸ¬ 1: `ModuleNotFoundError: No module named 'app'`

**ì›ì¸:** Pythonì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ì§€ ëª»í•¨

**í•´ê²°:**
```bash
# ë°©ë²• 1: PYTHONPATH ì„¤ì • (ì¶”ì²œ)
export PYTHONPATH="${PYTHONPATH}:$(pwd)"  # Mac/Linux
set PYTHONPATH=%PYTHONPATH%;%CD%  # Windows

# ë°©ë²• 2: ëª¨ë“ˆë¡œ ì‹¤í–‰
python -m tests.test_parser
```

---

### ì—ëŸ¬ 2: `openai.error.AuthenticationError`

**ì›ì¸:** API í‚¤ê°€ ì˜ëª»ë¨

**í•´ê²°:**
1. `.env` íŒŒì¼ì—ì„œ `OPENAI_API_KEY` í™•ì¸
2. https://platform.openai.com/api-keys ì—ì„œ í‚¤ ì¬ë°œê¸‰
3. í‚¤ ì•ë’¤ ê³µë°± ì œê±° í™•ì¸
4. ì„œë²„ ì¬ì‹œì‘ í•„ìˆ˜!

---

### ì—ëŸ¬ 3: `ImportError: cannot import name 'OpenAIEmbeddings'`

**ì›ì¸:** LangChain ë²„ì „ ë¶ˆì¼ì¹˜

**í•´ê²°:**
```bash
pip uninstall langchain langchain-openai -y
pip install langchain==0.1.0 langchain-openai==0.0.2
```

---

### ì—ëŸ¬ 4: ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ (ë¹ˆ ë°°ì—´)

**ì›ì¸:** 
- ë²¡í„° DBê°€ ë¹„ì–´ìˆê±°ë‚˜
- `score_threshold`ê°€ ë„ˆë¬´ ë†’ìŒ

**í•´ê²°:**
```python
# app/vector_store.pyì—ì„œ ì„ê³„ì¹˜ ë‚®ì¶”ê¸°
results = vector_client.similarity_search_with_relevance_scores(
    query=query,
    k=k,
    score_threshold=0.5  # 0.75 â†’ 0.5ë¡œ ë‚®ì¶¤
)
```

**ë””ë²„ê¹…:**
```python
# ë²¡í„° DBì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
print(f"ìƒ‰ì¸ëœ ë¬¸ì„œ ìˆ˜: {vector_client._collection.count()}")
```

---

### ì—ëŸ¬ 5: PDF íŒŒì‹± ì‹œ í•œê¸€ ê¹¨ì§

**ì›ì¸:** PDF ì¸ì½”ë”© ë¬¸ì œ

**í•´ê²°:**
```python
# app/parsers/pdf.py ìˆ˜ì •
text = "\n\n".join(
    page.extract_text(extraction_mode="layout")  # layout ëª¨ë“œ ì¶”ê°€
    for page in reader.pages
)
```

**ëŒ€ì•ˆ:** `pdfplumber` ì‚¬ìš©
```bash
pip install pdfplumber
```

```python
import pdfplumber

def parse_pdf(file_path):
    with pdfplumber.open(file_path) as pdf:
        text = "\n\n".join(page.extract_text() for page in pdf.pages)
    ...
```


---

## âœ… ë‹¨ê³„ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1: ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

**Step 1-2: í™˜ê²½ ì„¤ì • (30ë¶„)**
- [ ] í”„ë¡œì íŠ¸ í´ë” ìƒì„± ì™„ë£Œ (`app/`, `data/`, `tests/` ë“±)
- [ ] ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸ (í”„ë¡¬í”„íŠ¸ì— `(.venv)` í‘œì‹œ)
- [ ] `requirements.txt` ì„¤ì¹˜ ì™„ë£Œ (ì—ëŸ¬ ì—†ìŒ)
- [ ] `.env` íŒŒì¼ ìƒì„± ë° `OPENAI_API_KEY` ì…ë ¥

**Step 3-4: ë¬¸ì„œ íŒŒì„œ (1ì‹œê°„)**
- [ ] `app/parsers/pdf.py` ì‘ì„±
- [ ] `tests/test_parser.py` ì‹¤í–‰ ì„±ê³µ
- [ ] ìƒ˜í”Œ PDF(10í˜ì´ì§€ ì´ìƒ) ì¤€ë¹„ ë° `data/` í´ë”ì— ë°°ì¹˜
- [ ] íŒŒì‹±ëœ í…ìŠ¤íŠ¸ 1,000ì ì´ìƒ í™•ì¸

**Step 5: ì²­í¬ ë¶„í•  (30ë¶„)**
- [ ] `app/pipelines.py` ì‘ì„±
- [ ] `tests/test_chunking.py` ì‹¤í–‰ â†’ ì²­í¬ ê°œìˆ˜ 10ê°œ ì´ìƒ
- [ ] ì²­í¬ ì˜¤ë²„ë© í™•ì¸ (ì²­í¬1 ë 50ìê°€ ì²­í¬2 ì‹œì‘ì— í¬í•¨)

**Step 6: ë²¡í„° DB ìƒ‰ì¸ (1ì‹œê°„)**
- [ ] `app/vector_store.py` ì‘ì„±
- [ ] `tests/test_full_pipeline.py` ì‹¤í–‰ â†’ "ìƒ‰ì¸ ì™„ë£Œ" ë©”ì‹œì§€
- [ ] `chroma/` í´ë” ìƒì„± í™•ì¸ (íŒŒì¼ í¬ê¸° 100KB ì´ìƒ)
- [ ] ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ â†’ ìœ ì‚¬ë„ 0.7 ì´ìƒ ë¬¸ì„œ 3ê°œ ì´ìƒ ë°œê²¬

**Step 7-8: GPT ë‹µë³€ & API (2ì‹œê°„)**
- [ ] `app/llm.py` ì‘ì„±
- [ ] `tests/test_answer.py` ì‹¤í–‰ â†’ ì¶œì²˜ í¬í•¨ëœ ë‹µë³€ ìƒì„±
- [ ] `app/main.py` ì‘ì„± ë° ì„œë²„ ì‹¤í–‰ (`python app/main.py`)
- [ ] http://localhost:8000/docs ì ‘ì† â†’ Swagger UI í™•ì¸
- [ ] `/ask` ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ â†’ 200 OK ì‘ë‹µ

---

### Week 2-3: ë°ëª¨ ì¤€ë¹„ (ì±—ë´‡ ì—°ë™ ì „)

- [ ] ì›¹ ë°ëª¨ UI ì‘ì„± (`web/index.html`)
- [ ] ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (API í‚¤ ì—†ì„ ë•Œ, ë¬¸ì„œ ì—†ì„ ë•Œ)
- [ ] ìŠ¤í¬ë¦°ìƒ· 10ì¥ ì´ìƒ ì´¬ì˜ (ì •ìƒ ì‘ë‹µ, ì—ëŸ¬, ì¶œì²˜ í‘œì‹œ)
- [ ] GitHubì— ì½”ë“œ ì—…ë¡œë“œ (`.env`ëŠ” ì œì™¸!)

---

### ìµœì¢… ê²€ì¦ (í”„ë¦¬ëœì„œ ì¤€ë¹„)

**ê¸°ìˆ  ê²€ì¦**
- [ ] ë‹¤ë¥¸ PDF 3ê°œë¡œ ì¬í…ŒìŠ¤íŠ¸ â†’ ëª¨ë‘ ì •ìƒ ìƒ‰ì¸
- [ ] 5ê°€ì§€ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ â†’ ì •í™•í•œ ì¶œì²˜ í‘œì‹œ
- [ ] ì‘ë‹µ ì‹œê°„ 10ì´ˆ ì´í•˜ (ê²€ìƒ‰ + GPT)
- [ ] ë¹„ìš© ê³„ì‚° ì •í™• (OpenAI ëŒ€ì‹œë³´ë“œì™€ ì¼ì¹˜)

**ë°ëª¨ ìë£Œ**
- [ ] 1ë¶„ ë°ëª¨ ì˜ìƒ ì´¬ì˜ (ì—…ë¡œë“œ â†’ ì§ˆë¬¸ â†’ ë‹µë³€ ì¶œì²˜)
- [ ] ë…¸ì…˜ ì†Œê°œ í˜ì´ì§€ ì‘ì„± ("RAG ì±—ë´‡ í¬íŠ¸í´ë¦¬ì˜¤")
- [ ] GitHub READMEì— ì„¤ì¹˜/ì‹¤í–‰ ê°€ì´ë“œ ì‘ì„±

**ì˜ì—… ì¤€ë¹„**
- [ ] ì œì•ˆì„œ í…œí”Œë¦¿ ì‘ì„± (ë¹„ìš©: 800~2,000ë§Œì›)
- [ ] ê¸°ìˆ  ìŠ¤íƒ ì„¤ëª… ìë£Œ (ë¹„ê°œë°œìë„ ì´í•´ ê°€ëŠ¥í•˜ê²Œ)
- [ ] FAQ 10ê°œ ì¤€ë¹„ ("í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ëŠ”?", "ë¹„ìš©ì€ ì–¼ë§ˆ?")

---

### ğŸš¨ í•„ìˆ˜ í™•ì¸ ì‚¬í•­

**ë³´ì•ˆ**
- [ ] `.gitignore`ì— `.env`, `chroma/`, `logs/` ì¶”ê°€
- [ ] GitHubì— API í‚¤ê°€ ë…¸ì¶œë˜ì§€ ì•Šì•˜ëŠ”ì§€ ì¬í™•ì¸
- [ ] ë°ëª¨ ì˜ìƒì— ì‹¤ì œ íšŒì‚¬ ë¬¸ì„œ ë…¸ì¶œ ì•ˆ ë¨

**ë¹„ìš©**
- [ ] OpenAI ì‚¬ìš©ëŸ‰ ì•Œë¦¼ ì„¤ì • (ì›” 10ë‹¬ëŸ¬ ì´ˆê³¼ ì‹œ ì´ë©”ì¼)
- [ ] í…ŒìŠ¤íŠ¸ ì‹œ ë¬¸ì„œ 10ê°œ ì´í•˜ë¡œ ì œí•œ (ì„ë² ë”© ë¹„ìš© ì ˆê°)

**í’ˆì§ˆ**
- [ ] 5ê°€ì§€ ì´ìƒì˜ ì§ˆë¬¸ìœ¼ë¡œ ì •í™•ë„ í…ŒìŠ¤íŠ¸
- [ ] ì¶œì²˜ê°€ ì—†ëŠ” ë‹µë³€ì€ ë°˜í™˜í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
- [ ] í•œê¸€ ë¬¸ì„œ íŒŒì‹± ì •ìƒ (ê¹¨ì§ ì—†ìŒ)

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ & í’ˆì§ˆ ê´€ë¦¬

| í…ŒìŠ¤íŠ¸        | ëª©ì                          | ëª…ë ¹                                                 |
| ------------- | ---------------------------- | ---------------------------------------------------- |
| ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ | ì—…ë¡œë“œ â†’ ì§ˆë¬¸ íë¦„ ì •ìƒ ë™ì‘ | `pytest tests/test_pipeline.py -m smoke`             |
| í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ | ì •í™•ë„Â·ê·¼ê±° ì¼ì¹˜ìœ¨ í™•ì¸      | `python scripts/evaluate.py --dataset data/eval.csv` |
| ë¶€í•˜ í…ŒìŠ¤íŠ¸   | ë‹¤ì¤‘ ìš”ì²­ ëŒ€ì‘               | `locust -f scripts/locustfile.py` (ì„ íƒ)             |

CI ì˜ˆì‹œ:
```yaml
- name: Pipeline Tests
  run: |
    pip install -r requirements.txt
    VECTOR_DB_PATH=./tmp/chroma pytest tests/ -m "pipeline"
```

---

## ğŸ” ìš´ì˜ íŒ

- ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ í›„ì—ëŠ” ë°˜ë“œì‹œ ì¬ìƒ‰ì¸ & ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
- Chroma í´ë”ëŠ” ì£¼ 1íšŒ ì´ìƒ ë°±ì—… (ZIP â†’ S3/Google Drive)
- ë¬¸ì„œ ë²„ì „ ê´€ë¦¬(Revision í•„ë“œ)ë¥¼ ì¶”ê°€í•´ ë¡¤ë°±ì„ ëŒ€ë¹„
- ìì£¼ ì§ˆë¬¸ë˜ëŠ” ë¬¸ì„œëŠ” `sources` í•„ë“œì— ë§í¬ë¥¼ ì¶”ê°€í•´ ì‚¬ìš©ìê°€ ë°”ë¡œ ì—´ëŒ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ëŠ”?

### Week 2: í…”ë ˆê·¸ë¨ ì±—ë´‡ ì—°ë™
â†’ [`implementation/bots.md`](bots.md)ë¡œ ì´ë™

**ë°°ìš¸ ë‚´ìš©:**
- BotFatherë¡œ ë´‡ ìƒì„±
- Webhook ì„¤ì • (ngrok ì‚¬ìš©)
- í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í¬ë§· ë³€í™˜
- 1ë¶„ ë°ëª¨ ì˜ìƒ ì´¬ì˜

---

### Week 3: ì›¹ ë°ëª¨ UI
â†’ [`implementation/web-admin.md`](web-admin.md)ë¡œ ì´ë™

**ë°°ìš¸ ë‚´ìš©:**
- HTML + Vanilla JSë¡œ ê¹”ë”í•œ UI
- ë¡œë”© ìŠ¤í”¼ë„ˆ & ì—ëŸ¬ í† ìŠ¤íŠ¸
- ì¶œì²˜ ë§í¬ ì¹´ë“œ
- ìŠ¤í¬ë¦°ìƒ· 10ì¥ í™•ë³´

---

### Week 4-6: í’ˆì§ˆ & ìš´ì˜
â†’ [`roadmap/week01-08.md`](../roadmap/week01-08.md)ë¡œ ì´ë™

**ë°°ìš¸ ë‚´ìš©:**
- ì •í™•ë„ í‰ê°€ (RAGAS)
- ê¶Œí•œ ê´€ë¦¬ (JWT)
- ë¹„ìš©/ë¡œê·¸ ëª¨ë‹ˆí„°ë§
- ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ

---

### Week 7-8: ë°°í¬ & ì˜ì—…
â†’ [`business/package.md`](../business/package.md)ë¡œ ì´ë™

**ë°°ìš¸ ë‚´ìš©:**
- Docker ë°°í¬
- ê°€ê²© íŒ¨í‚¤ì§€ êµ¬ì„±
- ì œì•ˆì„œ ì‘ì„±
- ê³„ì•½ì„œ í…œí”Œë¦¿

---

## ğŸ’¼ í”„ë¦¬ëœì„œ ìƒì¡´ íŒ

### ì²« ì™¸ì£¼ë¥¼ ë”°ë‚´ëŠ” ë²•

**1. í¬íŠ¸í´ë¦¬ì˜¤ 3ì¢… ì„¸íŠ¸**
```
âœ… GitHub ë¦¬í¬ì§€í† ë¦¬ (ì½”ë“œ + README)
âœ… ë…¸ì…˜ ì†Œê°œ í˜ì´ì§€ (ë¹„ê°œë°œììš©)
âœ… 1ë¶„ ë°ëª¨ ì˜ìƒ (YouTube ë¹„ê³µê°œ)
```

**2. ì œì•ˆì„œ ì‘ì„± ì˜ˆì‹œ**
```markdown
[íšŒì‚¬ëª…] ê·€í•˜

ì•ˆë…•í•˜ì„¸ìš”, RAG ì±—ë´‡ ì „ë¬¸ ê°œë°œì [ì´ë¦„]ì…ë‹ˆë‹¤.

ê·€ì‚¬ì˜ [ë¬¸ì œ ìƒí™© - ì˜ˆ: 200í˜ì´ì§€ ê·œì •ì§‘ ê²€ìƒ‰ ì–´ë ¤ì›€]ì„ 
AI ì±—ë´‡ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ì†”ë£¨ì…˜ì„ ì œì•ˆë“œë¦½ë‹ˆë‹¤.

ğŸ“Œ í•´ê²° ë°©ì•ˆ:
- PDF ì—…ë¡œë“œ â†’ ìë™ ìƒ‰ì¸
- ì§ˆë¬¸ ì‹œ ì¶œì²˜ í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ
- í…”ë ˆê·¸ë¨/ì›¹ ë©€í‹° ì±„ë„ ì§€ì›

ğŸ’° ì˜ˆìƒ ë¹„ìš©: 1,200ë§Œì› (4ì£¼ ê°œë°œ + 2ì£¼ í…ŒìŠ¤íŠ¸)
ğŸ“… ì¼ì •: ê³„ì•½ í›„ 6ì£¼
ğŸ¬ ë°ëª¨: [YouTube ë§í¬]

ê°ì‚¬í•©ë‹ˆë‹¤.
```

**3. ë‹¨ê°€ í˜‘ìƒ ê°€ì´ë“œ**
| êµ¬ë¶„      | ê¸ˆì•¡          | ê·¼ê±°                           |
| --------- | ------------- | ------------------------------ |
| ìµœì†Œ ê²¬ì  | 800ë§Œì›       | ê°œë°œ 4ì£¼ (ì£¼ 200ë§Œì›)          |
| í‘œì¤€ ê²¬ì  | 1,200ë§Œì›     | ê°œë°œ + í…ŒìŠ¤íŠ¸ + ë¬¸ì„œí™”         |
| ê³ ê¸‰ ê²¬ì  | 2,000ë§Œì›     | ê¶Œí•œ ê´€ë¦¬ + ê´€ë¦¬ì í˜ì´ì§€ í¬í•¨ |
| ìœ ì§€ë³´ìˆ˜  | ì›” 50~200ë§Œì› | ë¬¸ì„œ ì—…ë°ì´íŠ¸, ë²„ê·¸ ìˆ˜ì •       |

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [LangChain í…ìŠ¤íŠ¸ ë¶„í• ](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [Chroma ë¬¸ì„œ](https://docs.trychroma.com/)
- [OpenAI ê°€ê²©í‘œ](https://openai.com/pricing)
- [FastAPI íŠœí† ë¦¬ì–¼](https://fastapi.tiangolo.com/tutorial/)

### ì¶”ì²œ í•™ìŠµ
- [RAG ê°œë… ì˜ìƒ (YouTube)](https://youtube.com) - "Retrieval Augmented Generation ì„¤ëª…"
- [LangChain Cookbook](https://python.langchain.com/cookbook) - ì‹¤ì „ ì˜ˆì œ
- [Chroma íŠœí† ë¦¬ì–¼](https://docs.trychroma.com/getting-started) - 10ë¶„ ì…ë¬¸

### ë¬¸ì œ í•´ê²°
- [`appendix/troubleshooting.md`](../appendix/troubleshooting.md) - ìì£¼ ë¬»ëŠ” ì§ˆë¬¸
- GitHub Discussions - ì»¤ë®¤ë‹ˆí‹° ì§ˆë¬¸
- Stack Overflow #langchain #rag

---

## ğŸ† ë§ˆì§€ë§‰ ë‹¹ë¶€

**ì´ˆë³´ìê°€ ê°€ì¥ ë§ì´ í•˜ëŠ” ì‹¤ìˆ˜:**

1. âŒ **ì½”ë“œë§Œ ì‘ì„±í•˜ê³  í…ŒìŠ¤íŠ¸ ì•ˆ í•¨**
   - âœ… ê° Stepë§ˆë‹¤ ë°˜ë“œì‹œ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰í•˜ê¸°

2. âŒ **ë¬¸ì„œ í•˜ë‚˜ë¡œë§Œ í…ŒìŠ¤íŠ¸**
   - âœ… ìµœì†Œ 3ê°€ì§€ ì¢…ë¥˜ì˜ ë¬¸ì„œë¡œ ê²€ì¦ (PDF, Docx, í•œê¸€/ì˜ë¬¸)

3. âŒ **ì—ëŸ¬ ë‚˜ë©´ í¬ê¸°**
   - âœ… "ìì£¼ ë§Œë‚˜ëŠ” ì—ëŸ¬" ì„¹ì…˜ ë¨¼ì € í™•ì¸ í›„ êµ¬ê¸€ë§

4. âŒ **ì™„ë²½í•˜ê²Œ ë§Œë“¤ë ¤ê³  í•¨**
   - âœ… Week 1ì—ëŠ” "ì‘ë™ë§Œ í•˜ë©´ OK" â†’ ë‚˜ì¤‘ì— ë¦¬íŒ©í† ë§

5. âŒ **í˜¼ì ë™ë™ ì•“ìŒ**
   - âœ… GitHub Discussionsë‚˜ ì¹´ì¹´ì˜¤í†¡ ì˜¤í”ˆì±„íŒ…ìœ¼ë¡œ ì§ˆë¬¸

---

**ğŸ‰ ì—¬ê¸°ê¹Œì§€ ì™”ë‹¤ë©´, ë‹¹ì‹ ì€ ì´ë¯¸ 80%ì˜ ê°œë°œìë¥¼ ì•ì„°ìŠµë‹ˆë‹¤!**

ë‹¤ìŒ ì£¼ì°¨ë¡œ ë„˜ì–´ê°€ê¸° ì „ì—:
- [ ] ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¬í™•ì¸
- [ ] GitHubì— ì½”ë“œ í‘¸ì‹œ
- [ ] ë…¸ì…˜ì— í•™ìŠµ ì¼ì§€ ê¸°ë¡

**ë‹¤ìŒ ë‹¨ê³„:** [`implementation/bots.md`](bots.md)ì—ì„œ í…”ë ˆê·¸ë¨ ì±—ë´‡ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤! ğŸš€

