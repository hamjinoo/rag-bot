from app.parsers import parse_document
from app.pipelines import split_text
from app.vector_store import add_documents, search_documents

def test_full_pipeline():
    print("ğŸ“‚ Step 1: PDF íŒŒì‹±")
    text, metadata = parse_document("data/sample.pdf")
    print(f"   âœ… {len(text)}ì ì¶”ì¶œ\n")
    
    print("ğŸ”ª Step 2: ì²­í¬ ë¶„í• ")
    chunks = split_text(text, metadata)
    print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±\n")
    
    print("ğŸ—„ï¸ Step 3: ë²¡í„° DB ìƒ‰ì¸ (ì„ë² ë”© ìƒì„± ì¤‘... 30ì´ˆ ì†Œìš”)")
    # í…ŒìŠ¤íŠ¸ ì‹œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
    add_documents(chunks, clear_existing=True)
    print(f"   âœ… ìƒ‰ì¸ ì™„ë£Œ\n")
    
    print("ğŸ” Step 4: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    # ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë“¤ë¡œ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ì—¬ê¸°ì„œ ì œê°€ ë¬´ì—‡ì„ ë°°ìš¸ ìˆ˜ ìˆë‚˜ìš”?",
    ]
    
    for query in test_queries:
        print(f"\n   ì§ˆë¬¸: {query}")
        results = search_documents(query, k=5, score_threshold=None)  # k=5ë¡œ ëŠ˜ë ¤ì„œ ë” ë§ì€ ê²°ê³¼ í™•ì¸
        if results:
            # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
            unique_results = []
            seen = set()
            for doc, score in results:
                content_preview = doc.page_content[:50]
                if content_preview not in seen:
                    seen.add(content_preview)
                    unique_results.append((doc, score))
                    if len(unique_results) >= 3:
                        break
            
            print(f"   âœ… {len(unique_results)}ê°œ ê³ ìœ  ë¬¸ì„œ ë°œê²¬:")
            for i, (doc, score) in enumerate(unique_results, 1):
                preview = doc.page_content[:80].replace("\n", " ")
                print(f"      {i}. ìœ ì‚¬ë„ {score:.3f}: {preview}...")
        else:
            print("   âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    test_full_pipeline()
